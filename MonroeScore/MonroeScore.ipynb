{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Basic monroe engine:\n",
    "- Takes raw text\n",
    "- Tokenises\n",
    "- Stems words using Porter Stemmer (removes 'ing', 's'...\n",
    "- Sees how many match the (stemmed) most common 1000 words\n",
    "- Returns a float percentage score\n",
    "\n",
    "List of common words from http://www.ef.co.uk/english-resources/english-vocabulary/top-1000-words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\Emma\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Must run this cell first - downloads the specific nltk packages needed (the ones that don't get downloaded with pip)\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "    \n",
    "def tokenise(text):\n",
    "    '''\n",
    "    Takes raw text and returns a list of tokens\n",
    "    \n",
    "    At present, tokens are:\n",
    "    - 2+ characters long\n",
    "    - Alphabetic (numbers are automatically excluded)\n",
    "    '''\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    tokens = re.findall(\"[A-Za-z]{2,}\",text)\n",
    "    tokens = [t for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "def tag_proper_nouns(text):\n",
    "    '''\n",
    "    Takes raw text, tags it using a part-of-speech tagger and returns a list of the proper nouns\n",
    "    \n",
    "    Will be slow for large pieces of text!\n",
    "    \n",
    "    Requires averaged_perceptron_tagger from nltk (not all nltk packages are downloaded via pip)\n",
    "    can get this package by using nltk.download() in python shell\n",
    "    '''\n",
    "    proper_nouns = set()\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        tagged_sent = nltk.tag.pos_tag(tokenise(sent))\n",
    "        proper_nouns.update([word for word, pos in tagged_sent if pos == 'NNP'])\n",
    "    return proper_nouns\n",
    "\n",
    "def lowercase(tokens):\n",
    "    '''\n",
    "    Takes a list of tokens, makes them all lowercase and returns them\n",
    "    '''\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "def stem(tokens):\n",
    "    '''\n",
    "    Takes a list of tokens, applies a stemming algorithm (returns standardised forms of words - removes\n",
    "    'ing', 's'...) and returns a list of stemmed words\n",
    "    \n",
    "    At present:\n",
    "    - We use the Porter stemmer (least aggressive form of stemming - alternates are snowball and lancaster)\n",
    "    '''\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "def get_common():  \n",
    "    '''\n",
    "    Opens the text file containing the list of 1000 most common words found at\n",
    "    http://www.ef.co.uk/english-resources/english-vocabulary/top-1000-words/\n",
    "    removes the newlines and returns them as a list.\n",
    "    '''\n",
    "    text = []\n",
    "    with open('1000common.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.endswith('\\n'):\n",
    "                text.append(line[0:-1])\n",
    "            else:\n",
    "                text.append(line)\n",
    "    return text\n",
    "\n",
    "def read_file(filename):\n",
    "    '''\n",
    "    Open a file, read text, return a string\n",
    "    '''\n",
    "    text = ''\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.endswith('\\n'):\n",
    "                text+=line[0:-1]\n",
    "            else:\n",
    "                text+=line\n",
    "    return text\n",
    "\n",
    "\n",
    "def munroe_score(text, exclusions='', verbose=True):\n",
    "    '''\n",
    "    Takes raw text, tokenises and stems it, and compares the stems to the set of the stemmed 1000 most common words\n",
    "    Returns the percentage of words that were in the list of common words\n",
    "    \n",
    "    e.g. if output is 0.61, 61% of words were in the list of the 1000 most common. \n",
    "    '''\n",
    "    \n",
    "    # Process exclusions\n",
    "    if exclusions != '':\n",
    "        exclusions = lowercase(re.findall('\\w+', exclusions))\n",
    "    else:\n",
    "        exclusions = []\n",
    "    \n",
    "    # Find all words - alphanumeric strings not separated by punctuation of 1+ length\n",
    "    words = re.findall('\\w+', text)\n",
    "    \n",
    "    # Keep a record of how we tagged each item\n",
    "    tags = ['' for w in words]\n",
    "    \n",
    "    # Identify proper nouns\n",
    "    proper_nouns = tag_proper_nouns(text)\n",
    "    \n",
    "    # Tokenise and stem the words. Mark proper nouns and non-alphabetic words in the tag list.\n",
    "    tokens = []\n",
    "    for i, word in enumerate(words):\n",
    "        # Check if the word is a proper noun. If it is, mark it and put an empty string in the list of tokens\n",
    "        if word in proper_nouns:\n",
    "            tokens.append('')\n",
    "            tags[i] = 'proper noun' \n",
    "        elif word.lower() in exclusions:\n",
    "            tokens.append('')\n",
    "            tags[i] = 'excluded'             \n",
    "        else:\n",
    "            token = tokenise(word)\n",
    "            # If there is more than one token, it means the word was broken by a number, In this case, ignore it\n",
    "            # If there are no tokens, it means that there were no alphabetic characters in the token. Ignore it\n",
    "            if len(token) != 1:\n",
    "                tokens.append('')\n",
    "                tags[i] = 'not alphabetic'\n",
    "            else:\n",
    "                # Stem the word\n",
    "                tokens.append(stem(lowercase(token))[0])\n",
    "            \n",
    "    # Get the most common 1000 w~ords from the file\n",
    "    common = get_common()\n",
    "    \n",
    "    # Stem the words so that they match the form of our tokens\n",
    "    stemmed_common = set(stem(common))\n",
    "    \n",
    "    # Count the number of tokens that are in the most common 1000 words\n",
    "    munroe = 0\n",
    "    for i, t in enumerate(tokens):\n",
    "        if t != '':\n",
    "            if t in stemmed_common:\n",
    "                munroe+=1\n",
    "                tags[i] = 'common'\n",
    "            else:\n",
    "                tags[i] = 'not common'\n",
    "    \n",
    "    \n",
    "    # If verbose, return some printed output\n",
    "    if verbose:\n",
    "        print('You have '+ str(len(stems)) + ' words in your document')\n",
    "        print('Of these, '+str(munroe)+' are in the most common 1000 words!')\n",
    "        print('Score: '+str(100*munroe/len([t for t in tokens if t != '']))+'%')\n",
    "        \n",
    "    return_dict = {\n",
    "        'score': munroe/len([t for t in tokens if t != '']),\n",
    "        'tagged_words': list(zip(words,tags))\n",
    "    }\n",
    "    return return_dict\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bioethics of human embryonic stem cell research (hESR) is controversial, including in Asia. After the 2001 US-moratorium on the federal funding of hESR, some Asian countries jumped into the 'bioethical vacuum', claiming that Asian countries do not suffer from Western religious scruples about using human embryos in research. Nevertheless, controversies around the donation of oocytes, the trade and barter of embryos, stem cell research trials, and human embryonic cloning in Asia have attracted global media attention. International guidelines are being adopted into diverging economic, political and socio-cultural contexts in Asia.\n",
      "\n",
      "\n",
      "This comparative research asks on what basis these guidelines are adopted in a socialist developing country such as China (PRC) and in a wealthy, democratic bureaucracy such as Japan. It investigates the formulation and implementation of regulations by visiting laboratories and clinics, interviewing donors of embryos and oocytes, observing scientists that handle the ‘materials’ and analysing public debates. Studying how bioethics guidelines created by governments, medical associations and private companies impact research and international research cooperation, the research expects to provide insights into how scientists, publics and governments deal with regulatory and bioethical problems in very different economic, political and cultural contexts.\n"
     ]
    }
   ],
   "source": [
    "text= \"\"\"The bioethics of human embryonic stem cell research (hESR) is controversial, including in Asia. After the 2001 US-moratorium on the federal funding of hESR, some Asian countries jumped into the 'bioethical vacuum', claiming that Asian countries do not suffer from Western religious scruples about using human embryos in research. Nevertheless, controversies around the donation of oocytes, the trade and barter of embryos, stem cell research trials, and human embryonic cloning in Asia have attracted global media attention. International guidelines are being adopted into diverging economic, political and socio-cultural contexts in Asia.\n",
    "\n",
    "\n",
    "This comparative research asks on what basis these guidelines are adopted in a socialist developing country such as China (PRC) and in a wealthy, democratic bureaucracy such as Japan. It investigates the formulation and implementation of regulations by visiting laboratories and clinics, interviewing donors of embryos and oocytes, observing scientists that handle the ‘materials’ and analysing public debates. Studying how bioethics guidelines created by governments, medical associations and private companies impact research and international research cooperation, the research expects to provide insights into how scientists, publics and governments deal with regulatory and bioethical problems in very different economic, political and cultural contexts.\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 184 words in your document\n",
      "Of these, 124 are in the most common 1000 words!\n",
      "Score: 68.50828729281768%\n"
     ]
    }
   ],
   "source": [
    "score_dict = munroe_score(text, exclusions='investigates, Socialist, federal', verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "tests = os.listdir('../Tests')\n",
    "for test in tests:\n",
    "    print(test)\n",
    "    test_text = read_file(test)\n",
    "    munroe_score(test_text, verbose=True)\n",
    "    print('-'*50)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', 'common'),\n",
       " ('bioethics', 'not common'),\n",
       " ('of', 'common'),\n",
       " ('human', 'common'),\n",
       " ('embryonic', 'not common'),\n",
       " ('stem', 'not common'),\n",
       " ('cell', 'common'),\n",
       " ('research', 'common'),\n",
       " ('hESR', 'not common'),\n",
       " ('is', 'not common'),\n",
       " ('controversial', 'not common'),\n",
       " ('including', 'common'),\n",
       " ('in', 'common'),\n",
       " ('Asia', 'proper noun'),\n",
       " ('After', 'common'),\n",
       " ('the', 'common'),\n",
       " ('2001', 'not alphabetic'),\n",
       " ('US', 'proper noun'),\n",
       " ('moratorium', 'not common'),\n",
       " ('on', 'common'),\n",
       " ('the', 'common'),\n",
       " ('federal', 'excluded'),\n",
       " ('funding', 'common'),\n",
       " ('of', 'common'),\n",
       " ('hESR', 'not common'),\n",
       " ('some', 'common'),\n",
       " ('Asian', 'not common'),\n",
       " ('countries', 'common'),\n",
       " ('jumped', 'not common'),\n",
       " ('into', 'common'),\n",
       " ('the', 'common'),\n",
       " ('bioethical', 'not common'),\n",
       " ('vacuum', 'not common'),\n",
       " ('claiming', 'common'),\n",
       " ('that', 'common'),\n",
       " ('Asian', 'not common'),\n",
       " ('countries', 'common'),\n",
       " ('do', 'common'),\n",
       " ('not', 'common'),\n",
       " ('suffer', 'common'),\n",
       " ('from', 'common'),\n",
       " ('Western', 'common'),\n",
       " ('religious', 'common'),\n",
       " ('scruples', 'not common'),\n",
       " ('about', 'common'),\n",
       " ('using', 'common'),\n",
       " ('human', 'common'),\n",
       " ('embryos', 'not common'),\n",
       " ('in', 'common'),\n",
       " ('research', 'common'),\n",
       " ('Nevertheless', 'not common'),\n",
       " ('controversies', 'not common'),\n",
       " ('around', 'common'),\n",
       " ('the', 'common'),\n",
       " ('donation', 'not common'),\n",
       " ('of', 'common'),\n",
       " ('oocytes', 'not common'),\n",
       " ('the', 'common'),\n",
       " ('trade', 'common'),\n",
       " ('and', 'common'),\n",
       " ('barter', 'not common'),\n",
       " ('of', 'common'),\n",
       " ('embryos', 'not common'),\n",
       " ('stem', 'not common'),\n",
       " ('cell', 'common'),\n",
       " ('research', 'common'),\n",
       " ('trials', 'common'),\n",
       " ('and', 'common'),\n",
       " ('human', 'common'),\n",
       " ('embryonic', 'not common'),\n",
       " ('cloning', 'not common'),\n",
       " ('in', 'common'),\n",
       " ('Asia', 'proper noun'),\n",
       " ('have', 'common'),\n",
       " ('attracted', 'not common'),\n",
       " ('global', 'not common'),\n",
       " ('media', 'common'),\n",
       " ('attention', 'common'),\n",
       " ('International', 'proper noun'),\n",
       " ('guidelines', 'not common'),\n",
       " ('are', 'not common'),\n",
       " ('being', 'common'),\n",
       " ('adopted', 'not common'),\n",
       " ('into', 'common'),\n",
       " ('diverging', 'not common'),\n",
       " ('economic', 'common'),\n",
       " ('political', 'common'),\n",
       " ('and', 'common'),\n",
       " ('socio', 'not common'),\n",
       " ('cultural', 'common'),\n",
       " ('contexts', 'not common'),\n",
       " ('in', 'common'),\n",
       " ('Asia', 'proper noun'),\n",
       " ('This', 'common'),\n",
       " ('comparative', 'common'),\n",
       " ('research', 'common'),\n",
       " ('asks', 'common'),\n",
       " ('on', 'common'),\n",
       " ('what', 'common'),\n",
       " ('basis', 'not common'),\n",
       " ('these', 'common'),\n",
       " ('guidelines', 'not common'),\n",
       " ('are', 'not common'),\n",
       " ('adopted', 'not common'),\n",
       " ('in', 'common'),\n",
       " ('a', 'not alphabetic'),\n",
       " ('socialist', 'excluded'),\n",
       " ('developing', 'common'),\n",
       " ('country', 'common'),\n",
       " ('such', 'common'),\n",
       " ('as', 'common'),\n",
       " ('China', 'proper noun'),\n",
       " ('PRC', 'proper noun'),\n",
       " ('and', 'common'),\n",
       " ('in', 'common'),\n",
       " ('a', 'not alphabetic'),\n",
       " ('wealthy', 'not common'),\n",
       " ('democratic', 'common'),\n",
       " ('bureaucracy', 'not common'),\n",
       " ('such', 'common'),\n",
       " ('as', 'common'),\n",
       " ('Japan', 'proper noun'),\n",
       " ('It', 'common'),\n",
       " ('investigates', 'excluded'),\n",
       " ('the', 'common'),\n",
       " ('formulation', 'not common'),\n",
       " ('and', 'common'),\n",
       " ('implementation', 'not common'),\n",
       " ('of', 'common'),\n",
       " ('regulations', 'not common'),\n",
       " ('by', 'common'),\n",
       " ('visiting', 'common'),\n",
       " ('laboratories', 'not common'),\n",
       " ('and', 'common'),\n",
       " ('clinics', 'not common'),\n",
       " ('interviewing', 'common'),\n",
       " ('donors', 'not common'),\n",
       " ('of', 'common'),\n",
       " ('embryos', 'not common'),\n",
       " ('and', 'common'),\n",
       " ('oocytes', 'not common'),\n",
       " ('observing', 'not common'),\n",
       " ('scientists', 'common'),\n",
       " ('that', 'common'),\n",
       " ('handle', 'not common'),\n",
       " ('the', 'common'),\n",
       " ('materials', 'common'),\n",
       " ('and', 'common'),\n",
       " ('analysing', 'not common'),\n",
       " ('public', 'common'),\n",
       " ('debates', 'common'),\n",
       " ('Studying', 'common'),\n",
       " ('how', 'common'),\n",
       " ('bioethics', 'not common'),\n",
       " ('guidelines', 'not common'),\n",
       " ('created', 'common'),\n",
       " ('by', 'common'),\n",
       " ('governments', 'common'),\n",
       " ('medical', 'common'),\n",
       " ('associations', 'not common'),\n",
       " ('and', 'common'),\n",
       " ('private', 'common'),\n",
       " ('companies', 'common'),\n",
       " ('impact', 'common'),\n",
       " ('research', 'common'),\n",
       " ('and', 'common'),\n",
       " ('international', 'common'),\n",
       " ('research', 'common'),\n",
       " ('cooperation', 'not common'),\n",
       " ('the', 'common'),\n",
       " ('research', 'common'),\n",
       " ('expects', 'common'),\n",
       " ('to', 'common'),\n",
       " ('provide', 'common'),\n",
       " ('insights', 'not common'),\n",
       " ('into', 'common'),\n",
       " ('how', 'common'),\n",
       " ('scientists', 'common'),\n",
       " ('publics', 'common'),\n",
       " ('and', 'common'),\n",
       " ('governments', 'common'),\n",
       " ('deal', 'common'),\n",
       " ('with', 'common'),\n",
       " ('regulatory', 'not common'),\n",
       " ('and', 'common'),\n",
       " ('bioethical', 'not common'),\n",
       " ('problems', 'common'),\n",
       " ('in', 'common'),\n",
       " ('very', 'common'),\n",
       " ('different', 'common'),\n",
       " ('economic', 'common'),\n",
       " ('political', 'common'),\n",
       " ('and', 'common'),\n",
       " ('cultural', 'common'),\n",
       " ('contexts', 'not common')]"
      ]
     },
     "execution_count": 192,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "score_dict['tagged_words']"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
