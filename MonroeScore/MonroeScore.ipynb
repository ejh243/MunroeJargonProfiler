{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "Basic monroe engine:\n",
    "- Takes raw text\n",
    "- Tokenises\n",
    "- Stems words using Porter Stemmer (removes 'ing', 's'...\n",
    "- Sees how many match the (stemmed) most common 1000 words\n",
    "- Returns a float percentage score\n",
    "\n",
    "List of common words from http://www.ef.co.uk/english-resources/english-vocabulary/top-1000-words/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Must run this cell first - downloads the specific nltk packages needed (the ones that don't get downloaded with pip)\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "    \n",
    "def tokenise(text):\n",
    "    '''\n",
    "    Takes raw text and returns a list of tokens\n",
    "    \n",
    "    At present, tokens are:\n",
    "    - 2+ characters long\n",
    "    - Alphabetic (numbers are automatically excluded)\n",
    "    '''\n",
    "    text = text.encode('ascii', 'ignore').decode('utf-8')\n",
    "    tokens = re.findall(\"[A-Za-z]{2,}\",text)\n",
    "    tokens = [t for t in tokens]\n",
    "    return tokens\n",
    "\n",
    "def tag_proper_nouns(text):\n",
    "    '''\n",
    "    Takes raw text, tags it using a part-of-speech tagger and returns a list of the proper nouns\n",
    "    \n",
    "    Will be slow for large pieces of text!\n",
    "    \n",
    "    Requires averaged_perceptron_tagger from nltk (not all nltk packages are downloaded via pip)\n",
    "    can get this package by using nltk.download() in python shell\n",
    "    '''\n",
    "    proper_nouns = set()\n",
    "    sentences = nltk.tokenize.sent_tokenize(text)\n",
    "    for sent in sentences:\n",
    "        tagged_sent = nltk.tag.pos_tag(tokenise(sent))\n",
    "        proper_nouns.update([word for word, pos in tagged_sent if pos == 'NNP'])\n",
    "    return proper_nouns\n",
    "\n",
    "def lowercase(tokens):\n",
    "    '''\n",
    "    Takes a list of tokens, makes them all lowercase and returns them\n",
    "    '''\n",
    "    return [t.lower() for t in tokens]\n",
    "\n",
    "def stem(tokens):\n",
    "    '''\n",
    "    Takes a list of tokens, applies a stemming algorithm (returns standardised forms of words - removes\n",
    "    'ing', 's'...) and returns a list of stemmed words\n",
    "    \n",
    "    At present:\n",
    "    - We use the Porter stemmer (least aggressive form of stemming - alternates are snowball and lancaster)\n",
    "    '''\n",
    "    stemmer = nltk.stem.PorterStemmer()\n",
    "    return [stemmer.stem(t) for t in tokens]\n",
    "\n",
    "def get_common():  \n",
    "    '''\n",
    "    Opens the text file containing the list of 1000 most common words found at\n",
    "    http://www.ef.co.uk/english-resources/english-vocabulary/top-1000-words/\n",
    "    removes the newlines and returns them as a list.\n",
    "    '''\n",
    "    text = []\n",
    "    with open('1000common.txt', 'r') as f:\n",
    "        for line in f:\n",
    "            if line.endswith('\\n'):\n",
    "                text.append(line[0:-1])\n",
    "            else:\n",
    "                text.append(line)\n",
    "    return text\n",
    "\n",
    "def read_file(filename):\n",
    "    '''\n",
    "    Open a file, read text, return a string\n",
    "    '''\n",
    "    text = ''\n",
    "    with open(filename, 'r') as f:\n",
    "        for line in f:\n",
    "            if line.endswith('\\n'):\n",
    "                text+=line[0:-1]\n",
    "            else:\n",
    "                text+=line\n",
    "    return text\n",
    "\n",
    "\n",
    "def munroe_score(text, verbose=True):\n",
    "    '''\n",
    "    Takes raw text, tokenises and stems it, and compares the stems to the set of the stemmed 1000 most common words\n",
    "    Returns the percentage of words that were in the list of common words\n",
    "    \n",
    "    e.g. if output is 0.61, 61% of words were in the list of the 1000 most common. \n",
    "    '''\n",
    "    \n",
    "    # Identify proper nouns\n",
    "    proper_nouns = tag_proper_nouns(text)\n",
    "    \n",
    "    # Tokenise text\n",
    "    all_tokens = tokenise(text)\n",
    "    \n",
    "    # Keep a record of how we tagged each item\n",
    "    tags = ['' for t in all_tokens]\n",
    "    \n",
    "    # Locate proper nouns in text and tag them\n",
    "    for i, token in enumerate(all_tokens):\n",
    "        if token in proper_nouns:\n",
    "            tags[i] = 'proper noun'\n",
    "    \n",
    "    # Remove proper nouns from text\n",
    "    for proper_noun in proper_nouns:        \n",
    "        tokens = [t for t in all_tokens if t != proper_noun]\n",
    "    \n",
    "    # Make the remaining tokens lowercase and stem them\n",
    "    stems = stem(lowercase(tokens))\n",
    "    \n",
    "    # Get the most common 1000 words from the file\n",
    "    common = get_common()\n",
    "    \n",
    "    # Stem the words so that they match the form of our tokens\n",
    "    stemmed_common = set(stem(common))\n",
    "    \n",
    "    # Count the number of tokens that are in the most common 1000 words\n",
    "    munroe = 0\n",
    "    for i, s in enumerate(stems):\n",
    "        if s in stemmed_common:\n",
    "            munroe+=1\n",
    "            tags[i] = 'common'\n",
    "        else:\n",
    "            tags[i] = 'not common'\n",
    "    \n",
    "    # If verbose, return some printed output\n",
    "    if verbose:\n",
    "        print('You have '+ str(len(stems)) + ' words in your document')\n",
    "        print('Of these, '+str(munroe)+' are in the most common 1000 words!')#\n",
    "        print('Score: '+str(munroe/len(stems))+'%')\n",
    "    return munroe/len(stems), list(zip(tokens,tags))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The bioethics of human embryonic stem cell research (hESR) is controversial, including in Asia. After the 2001 US-moratorium on the federal funding of hESR, some Asian countries jumped into the 'bioethical vacuum', claiming that Asian countries do not suffer from Western religious scruples about using human embryos in research. Nevertheless, controversies around the donation of oocytes, the trade and barter of embryos, stem cell research trials, and human embryonic cloning in Asia have attracted global media attention. International guidelines are being adopted into diverging economic, political and socio-cultural contexts in Asia.\n",
      "\n",
      "\n",
      "This comparative research asks on what basis these guidelines are adopted in a socialist developing country such as China (PRC) and in a wealthy, democratic bureaucracy such as Japan. It investigates the formulation and implementation of regulations by visiting laboratories and clinics, interviewing donors of embryos and oocytes, observing scientists that handle the ‘materials’ and analysing public debates. Studying how bioethics guidelines created by governments, medical associations and private companies impact research and international research cooperation, the research expects to provide insights into how scientists, publics and governments deal with regulatory and bioethical problems in very different economic, political and cultural contexts.\n"
     ]
    }
   ],
   "source": [
    "text= \"\"\"The bioethics of human embryonic stem cell research (hESR) is controversial, including in Asia. After the 2001 US-moratorium on the federal funding of hESR, some Asian countries jumped into the 'bioethical vacuum', claiming that Asian countries do not suffer from Western religious scruples about using human embryos in research. Nevertheless, controversies around the donation of oocytes, the trade and barter of embryos, stem cell research trials, and human embryonic cloning in Asia have attracted global media attention. International guidelines are being adopted into diverging economic, political and socio-cultural contexts in Asia.\n",
    "\n",
    "\n",
    "This comparative research asks on what basis these guidelines are adopted in a socialist developing country such as China (PRC) and in a wealthy, democratic bureaucracy such as Japan. It investigates the formulation and implementation of regulations by visiting laboratories and clinics, interviewing donors of embryos and oocytes, observing scientists that handle the ‘materials’ and analysing public debates. Studying how bioethics guidelines created by governments, medical associations and private companies impact research and international research cooperation, the research expects to provide insights into how scientists, publics and governments deal with regulatory and bioethical problems in very different economic, political and cultural contexts.\"\"\"\n",
    "print(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "You have 191 words in your document\n",
      "Of these, 127 are in the most common 1000 words!\n",
      "Score: 0.6649214659685864%\n"
     ]
    }
   ],
   "source": [
    "score, tags = munroe_score(text, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('The', <Tags.COMMON: 1>),\n",
       " ('bioethics', <Tags.NOT_COMMON: 2>),\n",
       " ('of', <Tags.COMMON: 1>),\n",
       " ('human', <Tags.COMMON: 1>),\n",
       " ('embryonic', <Tags.NOT_COMMON: 2>),\n",
       " ('stem', <Tags.NOT_COMMON: 2>),\n",
       " ('cell', <Tags.COMMON: 1>),\n",
       " ('research', <Tags.COMMON: 1>),\n",
       " ('hESR', <Tags.NOT_COMMON: 2>),\n",
       " ('is', <Tags.NOT_COMMON: 2>),\n",
       " ('controversial', <Tags.NOT_COMMON: 2>),\n",
       " ('including', <Tags.COMMON: 1>),\n",
       " ('in', <Tags.COMMON: 1>),\n",
       " ('Asia', <Tags.NOT_COMMON: 2>),\n",
       " ('After', <Tags.COMMON: 1>),\n",
       " ('the', <Tags.COMMON: 1>),\n",
       " ('US', <Tags.COMMON: 1>),\n",
       " ('moratorium', <Tags.NOT_COMMON: 2>),\n",
       " ('on', <Tags.COMMON: 1>),\n",
       " ('the', <Tags.COMMON: 1>),\n",
       " ('federal', <Tags.COMMON: 1>),\n",
       " ('funding', <Tags.COMMON: 1>),\n",
       " ('of', <Tags.COMMON: 1>),\n",
       " ('hESR', <Tags.NOT_COMMON: 2>),\n",
       " ('some', <Tags.COMMON: 1>),\n",
       " ('Asian', <Tags.NOT_COMMON: 2>),\n",
       " ('countries', <Tags.COMMON: 1>),\n",
       " ('jumped', <Tags.NOT_COMMON: 2>),\n",
       " ('into', <Tags.COMMON: 1>),\n",
       " ('the', <Tags.COMMON: 1>),\n",
       " ('bioethical', <Tags.NOT_COMMON: 2>),\n",
       " ('vacuum', <Tags.NOT_COMMON: 2>),\n",
       " ('claiming', <Tags.COMMON: 1>),\n",
       " ('that', <Tags.COMMON: 1>),\n",
       " ('Asian', <Tags.NOT_COMMON: 2>),\n",
       " ('countries', <Tags.COMMON: 1>),\n",
       " ('do', <Tags.COMMON: 1>),\n",
       " ('not', <Tags.COMMON: 1>),\n",
       " ('suffer', <Tags.COMMON: 1>),\n",
       " ('from', <Tags.COMMON: 1>),\n",
       " ('Western', <Tags.COMMON: 1>),\n",
       " ('religious', <Tags.COMMON: 1>),\n",
       " ('scruples', <Tags.NOT_COMMON: 2>),\n",
       " ('about', <Tags.COMMON: 1>),\n",
       " ('using', <Tags.COMMON: 1>),\n",
       " ('human', <Tags.COMMON: 1>),\n",
       " ('embryos', <Tags.NOT_COMMON: 2>),\n",
       " ('in', <Tags.COMMON: 1>),\n",
       " ('research', <Tags.COMMON: 1>),\n",
       " ('Nevertheless', <Tags.NOT_COMMON: 2>),\n",
       " ('controversies', <Tags.NOT_COMMON: 2>),\n",
       " ('around', <Tags.COMMON: 1>),\n",
       " ('the', <Tags.COMMON: 1>),\n",
       " ('donation', <Tags.NOT_COMMON: 2>),\n",
       " ('of', <Tags.COMMON: 1>),\n",
       " ('oocytes', <Tags.NOT_COMMON: 2>),\n",
       " ('the', <Tags.COMMON: 1>),\n",
       " ('trade', <Tags.COMMON: 1>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('barter', <Tags.NOT_COMMON: 2>),\n",
       " ('of', <Tags.COMMON: 1>),\n",
       " ('embryos', <Tags.NOT_COMMON: 2>),\n",
       " ('stem', <Tags.NOT_COMMON: 2>),\n",
       " ('cell', <Tags.COMMON: 1>),\n",
       " ('research', <Tags.COMMON: 1>),\n",
       " ('trials', <Tags.COMMON: 1>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('human', <Tags.COMMON: 1>),\n",
       " ('embryonic', <Tags.NOT_COMMON: 2>),\n",
       " ('cloning', <Tags.NOT_COMMON: 2>),\n",
       " ('in', <Tags.COMMON: 1>),\n",
       " ('Asia', <Tags.NOT_COMMON: 2>),\n",
       " ('have', <Tags.COMMON: 1>),\n",
       " ('attracted', <Tags.NOT_COMMON: 2>),\n",
       " ('global', <Tags.NOT_COMMON: 2>),\n",
       " ('media', <Tags.COMMON: 1>),\n",
       " ('attention', <Tags.COMMON: 1>),\n",
       " ('International', <Tags.COMMON: 1>),\n",
       " ('guidelines', <Tags.NOT_COMMON: 2>),\n",
       " ('are', <Tags.NOT_COMMON: 2>),\n",
       " ('being', <Tags.COMMON: 1>),\n",
       " ('adopted', <Tags.NOT_COMMON: 2>),\n",
       " ('into', <Tags.COMMON: 1>),\n",
       " ('diverging', <Tags.NOT_COMMON: 2>),\n",
       " ('economic', <Tags.COMMON: 1>),\n",
       " ('political', <Tags.COMMON: 1>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('socio', <Tags.NOT_COMMON: 2>),\n",
       " ('cultural', <Tags.COMMON: 1>),\n",
       " ('contexts', <Tags.NOT_COMMON: 2>),\n",
       " ('in', <Tags.COMMON: 1>),\n",
       " ('Asia', <Tags.NOT_COMMON: 2>),\n",
       " ('This', <Tags.COMMON: 1>),\n",
       " ('comparative', <Tags.COMMON: 1>),\n",
       " ('research', <Tags.COMMON: 1>),\n",
       " ('asks', <Tags.COMMON: 1>),\n",
       " ('on', <Tags.COMMON: 1>),\n",
       " ('what', <Tags.COMMON: 1>),\n",
       " ('basis', <Tags.NOT_COMMON: 2>),\n",
       " ('these', <Tags.COMMON: 1>),\n",
       " ('guidelines', <Tags.NOT_COMMON: 2>),\n",
       " ('are', <Tags.NOT_COMMON: 2>),\n",
       " ('adopted', <Tags.NOT_COMMON: 2>),\n",
       " ('in', <Tags.COMMON: 1>),\n",
       " ('socialist', <Tags.NOT_COMMON: 2>),\n",
       " ('developing', <Tags.COMMON: 1>),\n",
       " ('country', <Tags.COMMON: 1>),\n",
       " ('such', <Tags.COMMON: 1>),\n",
       " ('as', <Tags.COMMON: 1>),\n",
       " ('PRC', <Tags.NOT_COMMON: 2>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('in', <Tags.COMMON: 1>),\n",
       " ('wealthy', <Tags.NOT_COMMON: 2>),\n",
       " ('democratic', <Tags.COMMON: 1>),\n",
       " ('bureaucracy', <Tags.NOT_COMMON: 2>),\n",
       " ('such', <Tags.COMMON: 1>),\n",
       " ('as', <Tags.COMMON: 1>),\n",
       " ('Japan', <Tags.NOT_COMMON: 2>),\n",
       " ('It', <Tags.COMMON: 1>),\n",
       " ('investigates', <Tags.NOT_COMMON: 2>),\n",
       " ('the', <Tags.COMMON: 1>),\n",
       " ('formulation', <Tags.NOT_COMMON: 2>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('implementation', <Tags.NOT_COMMON: 2>),\n",
       " ('of', <Tags.COMMON: 1>),\n",
       " ('regulations', <Tags.NOT_COMMON: 2>),\n",
       " ('by', <Tags.COMMON: 1>),\n",
       " ('visiting', <Tags.COMMON: 1>),\n",
       " ('laboratories', <Tags.NOT_COMMON: 2>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('clinics', <Tags.NOT_COMMON: 2>),\n",
       " ('interviewing', <Tags.COMMON: 1>),\n",
       " ('donors', <Tags.NOT_COMMON: 2>),\n",
       " ('of', <Tags.COMMON: 1>),\n",
       " ('embryos', <Tags.NOT_COMMON: 2>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('oocytes', <Tags.NOT_COMMON: 2>),\n",
       " ('observing', <Tags.NOT_COMMON: 2>),\n",
       " ('scientists', <Tags.COMMON: 1>),\n",
       " ('that', <Tags.COMMON: 1>),\n",
       " ('handle', <Tags.NOT_COMMON: 2>),\n",
       " ('the', <Tags.COMMON: 1>),\n",
       " ('materials', <Tags.COMMON: 1>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('analysing', <Tags.NOT_COMMON: 2>),\n",
       " ('public', <Tags.COMMON: 1>),\n",
       " ('debates', <Tags.COMMON: 1>),\n",
       " ('Studying', <Tags.COMMON: 1>),\n",
       " ('how', <Tags.COMMON: 1>),\n",
       " ('bioethics', <Tags.NOT_COMMON: 2>),\n",
       " ('guidelines', <Tags.NOT_COMMON: 2>),\n",
       " ('created', <Tags.COMMON: 1>),\n",
       " ('by', <Tags.COMMON: 1>),\n",
       " ('governments', <Tags.COMMON: 1>),\n",
       " ('medical', <Tags.COMMON: 1>),\n",
       " ('associations', <Tags.NOT_COMMON: 2>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('private', <Tags.COMMON: 1>),\n",
       " ('companies', <Tags.COMMON: 1>),\n",
       " ('impact', <Tags.COMMON: 1>),\n",
       " ('research', <Tags.COMMON: 1>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('international', <Tags.COMMON: 1>),\n",
       " ('research', <Tags.COMMON: 1>),\n",
       " ('cooperation', <Tags.NOT_COMMON: 2>),\n",
       " ('the', <Tags.COMMON: 1>),\n",
       " ('research', <Tags.COMMON: 1>),\n",
       " ('expects', <Tags.COMMON: 1>),\n",
       " ('to', <Tags.COMMON: 1>),\n",
       " ('provide', <Tags.COMMON: 1>),\n",
       " ('insights', <Tags.NOT_COMMON: 2>),\n",
       " ('into', <Tags.COMMON: 1>),\n",
       " ('how', <Tags.COMMON: 1>),\n",
       " ('scientists', <Tags.COMMON: 1>),\n",
       " ('publics', <Tags.COMMON: 1>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('governments', <Tags.COMMON: 1>),\n",
       " ('deal', <Tags.COMMON: 1>),\n",
       " ('with', <Tags.COMMON: 1>),\n",
       " ('regulatory', <Tags.NOT_COMMON: 2>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('bioethical', <Tags.NOT_COMMON: 2>),\n",
       " ('problems', <Tags.COMMON: 1>),\n",
       " ('in', <Tags.COMMON: 1>),\n",
       " ('very', <Tags.COMMON: 1>),\n",
       " ('different', <Tags.COMMON: 1>),\n",
       " ('economic', <Tags.COMMON: 1>),\n",
       " ('political', <Tags.COMMON: 1>),\n",
       " ('and', <Tags.COMMON: 1>),\n",
       " ('cultural', <Tags.COMMON: 1>),\n",
       " ('contexts', <Tags.NOT_COMMON: 2>)]"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import os \n",
    "tests = os.listdir('../Tests')\n",
    "for test in tests:\n",
    "    print(test)\n",
    "    test_text = read_file(test)\n",
    "    munroe_score(test_text, verbose=True)\n",
    "    print('-'*50)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
